{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X28zqoct9wvx"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6cM9qG99wv-",
    "outputId": "2856c339-dc32-43e4-d877-ee1bcba848c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fasttext in /Users/akallaku/anaconda3/lib/python3.10/site-packages (0.9.2)\r\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from fasttext) (2.10.4)\r\n",
      "Requirement already satisfied: numpy in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from fasttext) (1.23.5)\r\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from fasttext) (65.6.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1itQb_kHZKq",
    "outputId": "459e9d7b-dd2c-48ac-ccbf-b82d0eeacc5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /Users/akallaku/anaconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: tqdm in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: requests[socks] in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: six in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: filelock in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRnK2mxcHZKr",
    "outputId": "a0ec1a62-5849-4519-db16-94d2cdc86af8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
      "To: /content/cc.en.300.bin.gz\n",
      "100%|██████████| 4.50G/4.50G [00:26<00:00, 169MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the FastText model file if it doesn't exist\n",
    "import os\n",
    "import gdown\n",
    "if not os.path.exists('cc.en.300.bin'):\n",
    "        url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz'\n",
    "        output = 'cc.en.300.bin.gz'\n",
    "        gdown.download(url, output, quiet=False)\n",
    "\n",
    "        with gzip.open(output, 'rb') as f_in:\n",
    "            with open('cc.en.300.bin', 'wb') as f_out:\n",
    "                f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "gmT4xRWDHZKs",
    "outputId": "ab7c24b8-467a-41ce-fc79-4a7f0d62c8cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "\n",
    "model_path = 'cc.en.300.bin'\n",
    "\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "# Get the word embeddings\n",
    "embeddings = model.get_input_matrix()\n",
    "\n",
    "# Create a dictionary to store the word embeddings\n",
    "documents = []\n",
    "word_embeddings = {}\n",
    "\n",
    "# Populate the dictionary with word embeddings\n",
    "for word, vector in zip(model.get_words(), embeddings):\n",
    "    word_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNOsq_5rHZKu",
    "outputId": "4b87002b-ccd7-4ad5-8734-fb8696f4ca16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "print(len(word_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42UASpXHXZzE",
    "outputId": "c74370d3-4a21-4ca3-8008-b1e5df2cf14c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import fasttext.util\\nimport gdown,gzip\\nimport os\\nimport math\\n\\ndef get_normalized_word_embeddings(word_embeddings):\\n    word_embedding_strings=[]\\n    \\n    for word in word_embeddings:\\n        vector = word_embeddings[word]\\n        squared_sum = sum(val ** 2 for val in vector)\\n        normalization_factor = math.sqrt(squared_sum)\\n        normalized_vector = [val / normalization_factor for val in vector]\\n        word_embeddings[word] = normalized_vector\\n        vector_str = \\' \\'.join(str(val) for val in normalized_vector)\\n        word_embedding_strings.append(word+\" \"+vector_str)\\n\\n    return word_embedding_strings\\n\\ndocuments = get_normalized_word_embeddings(word_embeddings)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kernel crashing when normalizing the fasttext word embeddings.\n",
    "'''import fasttext.util\n",
    "import gdown,gzip\n",
    "import os\n",
    "import math\n",
    "\n",
    "def get_normalized_word_embeddings(word_embeddings):\n",
    "    word_embedding_strings=[]\n",
    "    \n",
    "    for word in word_embeddings:\n",
    "        vector = word_embeddings[word]\n",
    "        squared_sum = sum(val ** 2 for val in vector)\n",
    "        normalization_factor = math.sqrt(squared_sum)\n",
    "        normalized_vector = [val / normalization_factor for val in vector]\n",
    "        word_embeddings[word] = normalized_vector\n",
    "        vector_str = ' '.join(str(val) for val in normalized_vector)\n",
    "        word_embedding_strings.append(word+\" \"+vector_str)\n",
    "\n",
    "    return word_embedding_strings\n",
    "\n",
    "documents = get_normalized_word_embeddings(word_embeddings)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Dn5UqMoluPIj",
    "outputId": "845fe24b-7f8b-481c-c756-4a636640391e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import nltk\\nnltk.download('stopwords')\\nfrom nltk.corpus import stopwords\\nstopword = stopwords.words('english')\\n\\ntemp_embeddings = word_embeddings.copy()\\nfor key in word_embeddings.keys():\\n    if key in stopword:\\n        del temp_embeddings[key]\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "temp_embeddings = word_embeddings.copy()\n",
    "for key in word_embeddings.keys():\n",
    "    if key in stopword:\n",
    "        del temp_embeddings[key]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-SVZevuYHZKw"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def pick_random_pairs(dictionary, num_pairs):\n",
    "    keys = list(dictionary.keys())\n",
    "    random.shuffle(keys)\n",
    "    random_pairs = {}\n",
    "    for key in keys[:num_pairs]:\n",
    "        random_pairs[key] = dictionary[key]\n",
    "    return random_pairs\n",
    "\n",
    "random_embeddings = pick_random_pairs(word_embeddings,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(random_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "flXXA-L9GSGX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normalize_word_embeddings(word_embeddings):\n",
    "    # Extract the word vectors and store them in a numpy array\n",
    "    embeddings = np.array(list(word_embeddings.values()))\n",
    "\n",
    "    # Normalize the word embeddings\n",
    "    normalized_embeddings = normalize(embeddings)\n",
    "\n",
    "    # Update the normalized embeddings back in the dictionary\n",
    "    for i, word in enumerate(word_embeddings.keys()):\n",
    "        word_embeddings[word] = normalized_embeddings[i]\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to get normalized word embeddings\n",
    "normalized_embeddings = normalize_word_embeddings(random_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Lw1zWZyWHZK1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "'''def embeddings_to_cosine_sim_matrix(E):\n",
    "    dot = E @ E.t()\n",
    "    norm = torch.norm(E, 2, 1)\n",
    "    x = torch.div(dot, norm)\n",
    "    x = torch.div(x, torch.unsqueeze(norm, 0))\n",
    "    return x\n",
    "\n",
    "def embeddings_to_cosine_sim_matrix(E, batch_size=50000):\n",
    "    num_embeddings = E.shape[0]\n",
    "    num_batches = (num_embeddings - 1) // batch_size + 1\n",
    "\n",
    "    cosine_sim_matrices = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, num_embeddings)\n",
    "\n",
    "        batch_embeddings = E[start_idx:end_idx]\n",
    "        dot = torch.matmul(batch_embeddings, E.t())\n",
    "        norm = torch.norm(E, 2, 1)\n",
    "        x = torch.div(dot, norm[:, None])\n",
    "        x = torch.div(x, norm[start_idx:end_idx].view(-1, 1))\n",
    "        cosine_sim_matrices.append(x)\n",
    "\n",
    "    return torch.cat(cosine_sim_matrices)'''\n",
    "\n",
    "def embeddings_to_cosine_sim_matrix(E):\n",
    "    cosine_sim_matrix = cosine_similarity(E, E)\n",
    "    return cosine_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bxULIrMdumuh"
   },
   "outputs": [],
   "source": [
    "word_embeddings_array = np.stack(list(normalized_embeddings.values()))\n",
    "\n",
    "# Convert the NumPy array to a torch tensor\n",
    "word_embeddings_tensor = torch.from_numpy(word_embeddings_array)\n",
    "\n",
    "# Call the embeddings_to_cosine_sim_matrix function\n",
    "cosine_sim_matrix = embeddings_to_cosine_sim_matrix(word_embeddings_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QeitKg2FjfUB"
   },
   "outputs": [],
   "source": [
    "# if needed\n",
    "# cosi = torch.load('./cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fu6H7yJF9wxl"
   },
   "outputs": [],
   "source": [
    "# This is the modification based on the Paus paper. Essentially, I am finding the max\n",
    "# number of boxes counted with each new box size where values are larger than 0 (because the\n",
    "# vectors were normalized before computing the dot product) and smaller than the size of the\n",
    "# area of the box. Once the max is found, the weights are added according to the paper where:\n",
    "# <= 0 == 0\n",
    "# >0 & <= max/3 == 1 * vector[index]\n",
    "# > max/3 & <= max/2 == 2 * vector[index]\n",
    "# > max/2 == 3 * vector[index]\n",
    "#\n",
    "#The result is then summed and the log of that sum is plotted against the log of 1 over the \n",
    "# size of the side of the box(1/sizes).\n",
    "# Then, I used linear regression to calulate the r^2 value and plot the log - log coorelation, \n",
    "# and found the fractal dimension using the polyfit function\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def fractal_dimension_weighted(Z, threshold=0.9):\n",
    "    \n",
    "    def boxcount(Z, k):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "                               np.arange(0, Z.shape[1], k), axis=1)\n",
    "        \n",
    "        # Count non-empty (0) and non-full boxes (k*k)\n",
    "        b_count = (np.where((S > 0) & (S < k*k))[0])\n",
    "        \n",
    "        #Find the maximum count of points between 0 and k^2\n",
    "        b_max = np.max(b_count)\n",
    "\n",
    "        \n",
    "        #Add weights per the Czech paper\n",
    "        for index in range(len(b_count)):\n",
    "        \n",
    "          if (b_count[index]) <= 0:\n",
    "              b_count[index]=0\n",
    "  \n",
    "          elif b_count[index] <= np.floor(b_max/3) and b_count[index] > 0:\n",
    "              b_count[index]=1*b_count[index] \n",
    "        \n",
    "          elif b_count[index] <= np.floor(b_max/2) and b_count[index] > np.floor(b_max/3):\n",
    "              b_count[index]=2*b_count[index] \n",
    "              \n",
    "          elif b_count[index] > b_max/2:\n",
    "              b_count[index]=3*b_count[index] \n",
    "              \n",
    "        return sum(b_count) \n",
    "        \n",
    "    # Transform Z utilizing the threshold\n",
    "    Z = (Z < threshold)\n",
    "\n",
    "    # Minimal dimension of matrix\n",
    "    p = min(Z.shape)\n",
    "\n",
    "    # Greatest power of 2 less than or equal to p\n",
    "    n = 2**np.floor(np.log(p)/np.log(2))\n",
    "\n",
    "    # Extract the exponent\n",
    "    n = int(np.log(n)/np.log(2))\n",
    "\n",
    "    # Build successive box sizes (from 2**n down to 2**1)\n",
    "    sizes = 2**np.arange(n, 1, -1)\n",
    "\n",
    "    # Actual box counting with decreasing size\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        counts.append(boxcount(Z, size))\n",
    "    \n",
    "    #Plot outputs\n",
    "    plt.plot(np.log(counts), np.log(1/sizes), color = \"g\")\n",
    "      \n",
    "    plt.xlabel('log(s)')\n",
    "    plt.ylabel('log(N(s))')\n",
    "      \n",
    "    plt.show()\n",
    "\n",
    "    # Find r^2\n",
    "\n",
    "    x = np.log(1/sizes)\n",
    "    y = np.log(counts)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    \n",
    "    print(\"r-squared:\", r_value**2)\n",
    "   \n",
    "    # Fit the successive log(sizes) with log (counts)\n",
    "    coeffs = np.polyfit(np.log(1/sizes), np.log(counts), 1)\n",
    "    return coeffs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZNmn1NELEqAx"
   },
   "outputs": [],
   "source": [
    "# This is the standard box counting method with no weights added and only counting the values\n",
    "# fall between 0 and k^2, where k is the size of the side of the box.\n",
    "def fractal_dimension(Z, threshold=0.9):\n",
    "    \n",
    "    def boxcount(Z, k):\n",
    "        S = np.add.reduceat(\n",
    "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
    "                               np.arange(0, Z.shape[1], k), axis=1)\n",
    "        \n",
    "        # Count non-empty (0) and non-full boxes (k*k)\n",
    "        return sum(np.where((S > 0) & (S < k*k))[0])\n",
    "        \n",
    "        \n",
    "    # Transform Z into a binary array\n",
    "    Z = (Z < threshold)\n",
    "\n",
    "    # Minimal dimension of matrix\n",
    "    p = min(Z.shape)\n",
    "\n",
    "    # Greatest power of 2 less than or equal to p\n",
    "    n = 2**np.floor(np.log(p)/np.log(2))\n",
    "\n",
    "    # Extract the exponent\n",
    "    n = int(np.log(n)/np.log(2))\n",
    "\n",
    "    # Build successive box sizes (from 2**n down to 2**1)\n",
    "    sizes = 2**np.arange(n, 1, -1)\n",
    "\n",
    "    # Actual box counting with decreasing size\n",
    "    counts = []\n",
    "    for size in sizes:\n",
    "        counts.append(boxcount(Z, size))\n",
    "    \n",
    "    #My additions\n",
    "    plt.plot(np.log(counts), np.log(1/sizes), color = \"g\")\n",
    "      \n",
    "    plt.xlabel('log(s)')\n",
    "    plt.ylabel('log(N(s))')\n",
    "      \n",
    "    plt.show()\n",
    "\n",
    "    x = np.log(1/sizes)\n",
    "    y = np.log(counts)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    \n",
    "    print(\"r-squared:\", r_value**2)\n",
    "   \n",
    "    # Fit the successive log(sizes) with log (counts)\n",
    "    coeffs = np.polyfit(np.log(1/sizes), np.log(counts), 1)\n",
    "    return coeffs[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xeKZY23EqAy",
    "outputId": "51cc8f8d-7fba-4aa7-a8d9-cd84c75e0a3b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "I = cosine_sim_matrix\n",
    "\n",
    "print(\"Minkowski–Bouligand dimension (computed): \", fractal_dimension(I))\n",
    "\n",
    "print(\"Minkowski–Bouligand dimension weighted(computed): \", fractal_dimension_weighted(I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
