{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X28zqoct9wvx"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6cM9qG99wv-",
    "outputId": "3b235704-4a97-4ebd-836a-d276ed6193db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.5.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge fasttext\n",
    "#use (!pip install fasttext) with collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y1itQb_kHZKq",
    "outputId": "f1b5c54f-3680-4e33-c373-3f9ad5902e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /Users/akallaku/anaconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: tqdm in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: six in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: filelock in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.14)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/akallaku/anaconda3/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRnK2mxcHZKr",
    "outputId": "10351240-7156-4cf2-ec41-533cbe1f16b5"
   },
   "outputs": [],
   "source": [
    "# Download the FastText model file if it doesn't exist\n",
    "import os\n",
    "import gdown\n",
    "if not os.path.exists('cc.en.300.bin'):\n",
    "        url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz'\n",
    "        output = 'cc.en.300.bin.gz'\n",
    "        gdown.download(url, output, quiet=False)\n",
    "\n",
    "        with gzip.open(output, 'rb') as f_in:\n",
    "            with open('cc.en.300.bin', 'wb') as f_out:\n",
    "                f_out.write(f_in.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmT4xRWDHZKs",
    "outputId": "b2a76d9d-660e-4251-c12c-b1b61b7310e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext.util\n",
    "\n",
    "model_path = 'cc.en.300.bin'\n",
    "\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "# Get the word embeddings\n",
    "embeddings = model.get_input_matrix()\n",
    "\n",
    "# Create a dictionary to store the word embeddings\n",
    "documents = []\n",
    "word_embeddings = {}\n",
    "\n",
    "# Populate the dictionary with word embeddings\n",
    "for word, vector in zip(model.get_words(), embeddings):\n",
    "    word_embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNOsq_5rHZKu",
    "outputId": "a8e70e43-dee5-40c2-bf93-54ac86f20487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000000\n"
     ]
    }
   ],
   "source": [
    "print(len(word_embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-SVZevuYHZKw"
   },
   "outputs": [],
   "source": [
    "def pick_word_embeddings(non_polysemous):\n",
    "    non_polysemous_embeddings = {}\n",
    "    for word in non_polysemous:\n",
    "        if(word in word_embeddings):\n",
    "            non_polysemous_embeddings[word] = word_embeddings[word]\n",
    "    \n",
    "    return non_polysemous_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_polysemous = [\n",
    "    \"banana\", \"guitar\", \"elephant\", \"chair\", \"diamond\", \"piano\", \"lemon\", \"mountain\", \"book\", \"sun\",\n",
    "    \"umbrella\", \"river\", \"butterfly\", \"tree\", \"carrot\", \"moon\", \"flower\", \"ocean\", \"computer\", \"lamp\",\n",
    "    \"coffee\", \"bird\", \"bicycle\", \"cookie\", \"beach\", \"dog\", \"rainbow\", \"camera\", \"island\", \"hat\",\n",
    "    \"turtle\", \"clock\", \"socks\", \"candle\", \"fire\", \"garden\", \"orange\", \"star\", \"bridge\", \"key\",\n",
    "    \"castle\", \"shoe\", \"dolphin\", \"planet\", \"spoon\", \"feather\", \"butter\", \"rocket\", \"pillow\", \"chocolate\",\n",
    "    \"honey\", \"volcano\", \"whale\", \"moonlight\", \"wallet\", \"pineapple\", \"flag\", \"fountain\", \"tiger\", \"map\",\n",
    "    \"sweater\", \"music\", \"airplane\", \"globe\", \"painting\", \"toothbrush\", \"helicopter\", \"snail\", \"statue\", \"cupcake\",\n",
    "    \"seashell\", \"peacock\", \"drum\", \"cloud\", \"cactus\", \"feather\", \"balloon\", \"kangaroo\", \"moonshine\", \"mailbox\",\n",
    "    \"raincoat\", \"pinecone\", \"lighthouse\", \"tornado\", \"volleyball\", \"seagull\", \"whistle\", \"accordion\", \"tadpole\", \"giraffe\",\n",
    "    \"typewriter\", \"caterpillar\", \"chimney\", \"waffle\", \"suitcase\", \"butterfly\", \"dragonfly\", \"toothpaste\", \"saxophone\", \"doorknob\"\n",
    "]\n",
    "\n",
    "non_polysemous_embeddings = pick_word_embeddings(non_polysemous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJs8ecLWNzgW",
    "outputId": "4cfa8221-e0e4-4d46-915c-d28080f168f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_polysemous_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "flXXA-L9GSGX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normalize_word_embeddings(word_embeddings):\n",
    "    # Extract the word vectors and store them in a numpy array\n",
    "    embeddings = np.array(list(word_embeddings.values()))\n",
    "\n",
    "    # Normalize the word embeddings\n",
    "    normalized_embeddings = normalize(embeddings)\n",
    "\n",
    "    # Update the normalized embeddings back in the dictionary\n",
    "    for i, word in enumerate(word_embeddings.keys()):\n",
    "        word_embeddings[word] = normalized_embeddings[i]\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "O8BHr5yyNzgY"
   },
   "outputs": [],
   "source": [
    "# Call the function to get normalized word embeddings\n",
    "normalized_embeddings = normalize_word_embeddings(non_polysemous_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QeitKg2FjfUB",
    "outputId": "d646be92-67ab-4703-b00e-3ecba583f459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.5.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge faiss\n",
    "#use (!pip install faiss-gpu) with collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "G3qfNg9ONzga"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "'''\n",
    "num_clusters = 256  \n",
    "quantizer = faiss.IndexFlatL2(embedding_vectors.shape[1])\n",
    "index = faiss.IndexIVFFlat(quantizer, embedding_vectors.shape[1], num_clusters)\n",
    "index.train(embedding_vectors)\n",
    "index.add(embedding_vectors)\n",
    "_, similar_indices = index.search(embedding_vectors, num_similar)\n",
    "\n",
    "'''\n",
    "\n",
    "def calculate_fractal_value(embeddings, box_size, k):\n",
    "    \n",
    "    embedding_vectors = np.array(list(embeddings.values()))\n",
    "    \n",
    "    # Initialize Faiss index\n",
    "    embedding_dim = embedding_vectors.shape[1]\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "    # Add embeddings to the Faiss index\n",
    "    index.add(embedding_vectors)\n",
    "\n",
    "    # Search for the nearest neighbors of all vectors\n",
    "    neighbor_distances , neighbor_indices = index.search(embedding_vectors, k)\n",
    "    \n",
    "    # Calculating similarity values using the neighbor_distances and updating it.\n",
    "    for i in range(neighbor_distances.shape[0]):\n",
    "        for j in range(neighbor_distances.shape[1]):\n",
    "            neighbor_distances[i][j] = 1/(1+neighbor_distances[i][j])\n",
    "    \n",
    "    #print(neighbor_distances)\n",
    "\n",
    "    #Resultant fractal dimension array\n",
    "    fractal_dimensions = []\n",
    "\n",
    "    for i in range(embedding_vectors.shape[0]):\n",
    "        num_boxes = 0\n",
    "        num_filled_boxes = 0\n",
    "\n",
    "        # Iterate over the similarity scores of each vector in chunks of box_size\n",
    "        for j in range(0,neighbor_distances.shape[1],box_size):\n",
    "            box_scores = neighbor_distances[i,j:j+box_size]\n",
    "            \n",
    "            for score in box_scores:\n",
    "                if(score>0.5):\n",
    "                    num_filled_boxes += 1\n",
    "\n",
    "            num_boxes+=1\n",
    "\n",
    "        fractal_dimension = np.log(num_filled_boxes) / np.log(num_boxes)\n",
    "        fractal_dimensions.append(fractal_dimension)\n",
    "\n",
    "    return fractal_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Y03oXI0RNzga"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractal Dimension: [inf, inf, inf, nan, nan, inf, inf, inf, nan, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, inf, inf, inf, nan, inf, inf, nan, nan, nan, inf, inf, inf, nan, inf, inf, nan, inf, inf, nan, inf, nan, inf, inf, inf, inf, nan, inf, inf, nan, nan, inf, inf, inf, inf, inf, inf, inf, nan, inf, inf, nan, inf, inf, inf, inf, nan, inf, inf, inf, inf, inf, inf, inf, nan, nan, inf, nan, nan, nan, nan, inf, inf, inf, nan, inf, inf, nan, inf, inf, inf, nan, inf, nan, nan, inf, inf, inf, inf, inf]\n",
      "CPU times: user 17 ms, sys: 4.5 ms, total: 21.4 ms\n",
      "Wall time: 22.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/yj4_93j15gsclb_cmx6t4n6hczt4j9/T/ipykernel_53805/3415109548.py:52: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  fractal_dimension = np.log(num_filled_boxes) / np.log(num_boxes)\n",
      "/var/folders/l7/yj4_93j15gsclb_cmx6t4n6hczt4j9/T/ipykernel_53805/3415109548.py:52: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  fractal_dimension = np.log(num_filled_boxes) / np.log(num_boxes)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fractal_dimensions = calculate_fractal_value(normalized_embeddings,10,10)\n",
    "print(\"Fractal Dimension:\", fractal_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Fractal_dimension_non_polysemous_words_100_10_10', 'w') as f:\n",
    "    for dimension in fractal_dimensions:\n",
    "        f.write(str(dimension) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "print(len(open('Fractal_dimension_non_polysemous_words_100_10_10', 'r').readlines()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
